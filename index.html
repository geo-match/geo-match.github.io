
<!DOCTYPE HTML>
<html>
    <head>
        <title>GeoMatch</title>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width">
        <link rel="stylesheet" href="https://use.typekit.net/quv7bsd.css"> <!-- fonts -->
        <link rel="stylesheet" href="flickity.min.css">
        <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
        <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/styles/default.min.css">
        <script src="flickity.pkgd.min.js"></script>
        <link rel="stylesheet" href="style.css" />
        <!-- Note: add back google analytics! -->
    </head>
    <body id="body">
        <div id="main"> 
            <header id="header"></header>
            <!-- style="padding-bottom:1em" -->
            <div id="profile" style="padding-top: 0px; margin-top: 0px; padding-bottom: 0px;margin-bottom: 0px">
                <!-- <img src="images/profile.jpg"> -->
                <div id="profile-desc" style="padding-top: 0px; margin-top: 0px; padding-bottom: 0px;margin-bottom: 0px">
                    <div id="profile-name" style="padding-top: 0px; margin-top: 0px; padding-bottom: 0px;margin-bottom: 0px">Geometry Matching for Multi-Embodiment Grasping</div>
                </div>
                <div style="clear: both;"></div>
            </div>

            <div class="container" id="main" style="padding-top: 0px; margin-top: 0px">        
                <div class="row" style="padding-top: 0px; margin-top: 0px">
                    <div class="col-md-12 text-center" style="padding-top: 0px; margin-top: 0px">
                        <ul class="list-inline" style="padding-top: 0px; margin-top: 0px">
                        <br>
                        <li><a href="https://jmattarian.com/">Maria Attarian</a></li>
                        <li><a href="https://www.cs.toronto.edu/~adilasif/">Muhammad Adil Asif</a></li>
                        <li>Jingzhou Liu</li>
                        <li>Ruthrash Hari</li>
                        <br>
                        <li><a href="https://animesh.garg.tech/">Animesh Garg</a></li>
                        <li><a href="https://www.gilitschenski.org/igor/">Igor Gilitschenski</a></li>
                        <li><a href="https://jonathantompson.github.io">Jonathan Tompson</a></li>
                        <br>
                             <!-- <a href="http://g.co/robotics">
                                <image src="images/robotics-at-google.png" height="40px" style="position: relative; top: 50%; transform: translateY(30%);"> 
                                <span>Robotics at Google</span>
                            </a>  -->
                        </ul>
                        <br>
                    </div>
                </div>
            </div>

            <div class="row justify-content-md-center">
            <div class="col-md-2 text-center">
                <a href="https://arxiv.org/abs/2312.03864" target="_blank">
                <image src="images/paper.png" height="60px">
                <!-- <image src="img/new.png" height="20px" class="imtip"> -->
                <h4 style="padding-bottom: 7px"><strong>Paper</strong></h4>
                </a>
            </div>
            <!-- <div class="col-md-2 text-center">
                <a href="" target="_blank">
                <image src="images/youtube_icon.png" height="60px">
                <image src="img/new.png" height="20px" class="imtip">
                <h4 style="padding-bottom: 7px"><strong>Video</strong></h4>
                </a>
            </div> -->
            <div class="col-md-2 text-center">
                <a href="https://github.com/google-deepmind/geomatch" target="_blank">
                <image src="images/github.png" height="60px">
                <!-- <image src="img/new.png" height="20px" class="imtip"> -->
                <h4 style="padding-bottom: 7px"><strong>Code</strong></h4>
                </a>
            </div>
            <!-- <li>
                        <a href="https://youtu.be/ysFav0b472w">
                        <image src="img/youtube_icon.png" height="60px">
                            <h4><strong>Video</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="https://ai.googleblog.com/2022/08/towards-helpful-robots-grounding.html">
                        <image src="img/google-ai-blog-small.png" height="60px">
                            <image src="img/new.png" height="20px" class="imtip">
                            <h4><strong>Blogpost</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="https://sites.research.google/palm-saycan">
                        <image src="img/demo.png" height="60px">
                            <image src="img/new.png" height="20px" class="imtip">
                            <h4><strong>Demo</strong></h4>
                        </a>
                    </li>  -->
            </div>

            <!-- Simple, centered version -->            

            <div class="section abstract">
            <h2 style="padding-top: 10px; margin-top: 10px"><b>Abstract</b></h2>
            <p> While significant progress has been made on the problem of generating grasps, many existing learning-based approaches still 
                concentrate on a single embodiment, provide limited generalization to higher DoF end-effectors and cannot capture a diverse set 
                of grasp modes. In this paper, we tackle the problem of grasping multi-embodiments through the viewpoint of learning 
                rich geometric representations for both objects and end-effectors using Graph Neural Networks (GNN). Our novel method - GeoMatch - applies 
                supervised learning on grasping data from multiple embodiments, learning end-to-end contact point likelihood maps as well as conditional 
                autoregressive prediction of grasps keypoint-by-keypoint. We compare our method against 3 baselines that provide multi-embodiment support. 
                Our approach performs better across 3 end-effectors, while also providing competitive diversity of grasps. 
                Examples can be found at <a href="geo-match.github.io">geo-match.github.io</a>.</p>
            </div>


            <div class="section paper">
            <h2 style="padding-top: 10px; margin-top: 10px"><b>Example grasps </b></h2>
                <p style="margin-bottom: 20px; margin-left: 30px">We train our method on a large number of diverse grasps spanning among 38 household objects and 5 different end-effectos: 2-finger, 3-finger, 4-finger, and 5-finger.</p>
                <div class="section recent-work">
                    <div class="highlight-proj" style="height: 310px; margin-bottom: 50px;">
                        <img src="images/qual_results.png">
                    </div>
                </div>
            </div>

            <div class="section paper">
            <h2 style="padding-top: 10px; margin-top: 10px"><b>GeoMatch: Method</b></h2>
                <p style="margin-bottom: 30px; margin-left: 30px">We propose a learning-based method that leverages GNN as generalized geometry encoders of objects and end-effectors and further predicts contacts for grasping in an autoregressive manner.
                <br>
                <br>
                <i>Our method architecture is the following: </i>
                </p>
                <div class="row">
                    <div class="col-sm">
                        <img style="width: 100%" src="images/arch_new.png">
                    </div>
                    <div class="col-sm">
                        <img style="width: 100%" src="images/ar_modules_new.png">
                    </div>
                </div>
                <p style="margin-bottom: 30px; margin-left: 30px">Point clouds of an object and end-effector are converted to graphs and passed through GNN 
                    encoders to obtain geometry embeddings that represent local geometry. The end-effector embeddings for a set of user selected keypoints 
                    are gathered and along with the object embedding, are used for contact prediction. This happens through an autoregressive setup where 
                    teacher forcing is applied to predict one keypoint at a time while also learning an independent distribution of object vertex to end-effector 
                    vertex contacts as an auxiliary task. Each autoregressive module is an MLP that operates on the difference map of all object points with 
                    the previous keypoint contact vertex, the end-effector keypoint embedding of the keypoint to be predicted, and the object embedding.  
            </div>
            <div class="section paper">
                <h2 style="padding-top: 10px; margin-top: 10px"><b>Demo</b></h2>
                <p>
                <video width="80%" playsinline="" muted="" autoplay="" loop="" controls>
                    <source src="video/real_demo.mp4" type="video/mp4">
                </video>
                </p>
                </div>
            
            <br><br><br><br><br><br><br><br><br><br>
        </div>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.4.2/jquery.js"></script>
    <script src="sync.js"></script>
    </body>
</html>
